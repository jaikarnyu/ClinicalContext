{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4WIqZyKBd_j",
        "outputId": "9f735b4d-d4fe-4fd9-a82e-24e09ddb7fbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m107.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill<0.3.7,>=0.3.0\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.14.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m609.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, aiosignal, aiohttp, datasets\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.12.0 dill-0.3.6 frozenlist-1.3.3 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.9.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.0.159-py3-none-any.whl (747 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m747.5/747.5 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7\n",
            "  Downloading dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.7)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.4)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.48.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.65.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.10)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Collecting marshmallow-enum<2.0.0,>=1.5.1\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect>=0.4.0\n",
            "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, tiktoken, openapi-schema-pydantic, marshmallow-enum, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.5.7 langchain-0.0.159 marshmallow-3.19.0 marshmallow-enum-1.5.1 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 tiktoken-0.3.3 typing-inspect-0.8.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.10/dist-packages (2022.12.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from dask) (2.2.1)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.10/dist-packages (from dask) (0.12.0)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from dask) (2023.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask) (6.0)\n",
            "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.10/dist-packages (from dask) (1.4.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from dask) (8.1.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask) (23.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.8)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.54.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=0.3.10->dask) (1.0.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.15.1-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.22.1-py2.py3-none-any.whl (203 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.1/203.1 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.27.1)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.3)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=fb215afa74ba3ef38379330a5e83b9fab2fed47420772d6a7a6177473b82c560\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.31 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.22.1 setproctitle-1.3.2 smmap-5.0.0 wandb-0.15.1\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install torch\n",
        "!pip install langchain tiktoken\n",
        "!pip install dask tensorflow\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h40HelE9xZkV"
      },
      "source": [
        "### Pre Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGGPPObkH7bU"
      },
      "source": [
        "### Start directly from here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 818,
          "referenced_widgets": [
            "72f0268a91584ce181c03f70a7365456",
            "9fed38a441f14e91aa2a854436870271",
            "1eb1f3ae5e914d77a5113bb6d05c248e",
            "d624042b78a04865ab8ece51a005ebc9",
            "99de6fee6d594e6eaeadf07888acc94c",
            "222fd374464941049dd750aaa8facf0f",
            "bdd2d05fbeec4f58b91d8c0c71fa1f25",
            "34bd1c6f0ab64c19bd297efae7e9e2d4",
            "9ac0ee618ba24517a237792cca0b1c2b",
            "f01f29e3fc174f39a44e2a539cf96043",
            "522bbc97683e41d2b3d1e838ea372694",
            "1f0d1a65cd7140e5ac95894b25ea836b",
            "3b85b6f868f3410ebae637b495f85f58",
            "61c20409fa784116a4e74ebe7a72f1fc",
            "ad3c90436af44019ba1813ea5e37f9d7",
            "0abfa64498674d9cbbdf4a05da3c3381",
            "918534600cb54d3288848fac2a20baf7"
          ]
        },
        "id": "lcw-K4ZOZh7X",
        "outputId": "533edc2a-74a4-41fe-8131-2b141a59a433"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "72f0268a91584ce181c03f70a7365456"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<wandb.jupyter.IFrame at 0x7fcb4cc2e800>"
            ],
            "text/html": [
              "<iframe src='https://wandb.ai/clinical-context/login/workspace?jupyter=true' style='border:none;width:100%;height:420px;'></iframe>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import datasets\n",
        "from datasets import Dataset\n",
        "from transformers import BertTokenizer, DataCollatorForLanguageModeling\n",
        "import pickle\n",
        "from transformers import AutoModelForMaskedLM, AutoTokenizer, TrainingArguments\n",
        "import pandas as pd\n",
        "import os\n",
        "# from torch.utils.data import Dataset\n",
        "import torch\n",
        "import tiktoken\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "import gc\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()\n",
        "\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "import math\n",
        "\n",
        "%wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqDbgI8TVbUk"
      },
      "outputs": [],
      "source": [
        "#  wandb.init(\n",
        "#       # set the wandb project where this run will be logged\n",
        "#       project=\"clinical-context\",\n",
        "      \n",
        "#       # track hyperparameters and run metadata\n",
        "#       config={\n",
        "#       \"chunk_size\": 128,\n",
        "#       \"architecture\": \"BIOBERT\",\n",
        "#       \"dataset\": \"MIMIC-4\",\n",
        "#       \"epochs\": 10,\n",
        "#       \"shard\" : 1\n",
        "#       }\n",
        "#   )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jChWaTjKnDn1"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def concat_columns(df):\n",
        "    # Concatenate all columns into a single column 'combined'\n",
        "    df['combined_text'] = df.apply(lambda x: ' '.join(x.astype(str)), axis=1)\n",
        "    \n",
        "    # Drop the original individual columns\n",
        "    df = df.drop(columns=df.columns[:-1])\n",
        "\n",
        "    # Rename the 'combined' column to 'text'\n",
        "    df = df.rename(columns={'combined': 'text'})\n",
        "    \n",
        "    return df\n",
        "\n",
        "def save_chunks(dataframe, chunk_size, output_dir):\n",
        "    print(\"Splitting dataframe split size %s\" %chunk_size)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    num_rows = len(dataframe)\n",
        "    num_chunks = (num_rows + chunk_size - 1) // chunk_size\n",
        "\n",
        "    for chunk_idx in range(num_chunks):\n",
        "        start_idx = chunk_idx * chunk_size\n",
        "        end_idx = min(start_idx + chunk_size, num_rows)\n",
        "\n",
        "        chunk_data = dataframe.iloc[start_idx:end_idx]\n",
        "        chunk_data.to_csv(os.path.join(output_dir, f\"chunk_{chunk_idx}.csv\"), index=False)\n",
        "        print(\"Split Saved : %s \" %chunk_idx)\n",
        "\n",
        "      \n",
        "\n",
        "def prepare_dataset(csv_file, output_dir, text_columns = ['text'], chunk_size=300, split_size=1000000):\n",
        "    \n",
        "    df = pd.read_csv(csv_file)\n",
        "    print(\"Processing file %s shape %s \" %(csv_file,df.shape))\n",
        "    columns_to_drop = [column for column in df.columns if column not in text_columns]\n",
        "    df.drop(columns_to_drop, axis=1)\n",
        "    print(\"Dropped non text columns. Remaining text columns %s \" %df.columns)\n",
        "    df = concat_columns(df)\n",
        "\n",
        "    chunk_size = 300\n",
        "\n",
        "    # text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=chunk_size, chunk_overlap=20)\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        # Set a really small chunk size, just to show.\n",
        "        chunk_size = chunk_size,\n",
        "        chunk_overlap  = 20,\n",
        "        length_function = len,\n",
        "    )\n",
        "\n",
        "    print(\"Chunking start\")\n",
        "\n",
        "    # Apply the split function to the 'text' column and store the resulting words in a list\n",
        "    chunks_list = df['text'].apply(lambda x: text_splitter.split_text(x))\n",
        "\n",
        "\n",
        "    # Flatten the list of lists\n",
        "    chunks = [chunk for sublist in chunks_list for chunk in sublist]\n",
        "\n",
        "    # Create a new DataFrame with the words\n",
        "    chunks_df = pd.DataFrame(chunks, columns=['text'])\n",
        "\n",
        "    print(\"Chunked data frame shape %s \" %chunks_df.shape)\n",
        "\n",
        "    split_size = 1000000  # Adjust this value according to your system's memory capacity\n",
        "    save_chunks(chunks_df, split_size, output_dir)\n",
        "    chunks_df.to_csv()\n",
        "    return chunks_df\n",
        "\n",
        "\n",
        "# Check token distribution\n",
        "def get_token_length(text):\n",
        "  # Global variables\n",
        "  tokenizer = tiktoken.get_encoding(\n",
        "      \"cl100k_base\"\n",
        "  )  # The encoding scheme to use for tokenization\n",
        "  return len(tokenizer.encode(text, disallowed_special=()))\n",
        "\n",
        "\n",
        "def tokenize_csv(chunk_file, tokenized_dir,split_size=500000):\n",
        "    os.makedirs(tokenized_dir, exist_ok=True)\n",
        "    print(\"Tokenizing start : file %s \" %chunk_file)\n",
        "    idx = 0\n",
        "    tokenized_files = os.listdir(tokenized_dir)\n",
        "    for chunk in pd.read_csv(chunk_file, chunksize=split_size):\n",
        "      tokenized_chunk_name = \"tokenized_{0}_{1}\".format(os.path.basename(chunk_file),idx)\n",
        "      if tokenized_chunk_name in tokenized_files:\n",
        "          print(\"Token data set already present for chunk. Skipping.\")\n",
        "          idx = idx + 1\n",
        "          continue\n",
        "      shard = Dataset.from_pandas(chunk)\n",
        "      tokenized_shard = shard.map(tokenize_function, batched=True, batch_size=32, num_proc=4)\n",
        "      tokenized_shard.save_to_disk(os.path.join(tokenized_dir, \"tokenized_{0}_{1}\".format(os.path.basename(chunk_file),idx)))\n",
        "      print(\"Tokenized chunk : idx %s : shape %s\" %(idx,chunk.shape))\n",
        "      idx = idx+1\n",
        "      try:\n",
        "        del chunk\n",
        "        del shard\n",
        "        del tokenized_shard\n",
        "        gc.collect()\n",
        "      except Exception as e:\n",
        "        pass \n",
        "    \n",
        "    print(\"Tokenizing complete : file %s\" %chunk_file)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDoNHOu5iiKN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-WePWOhdrcW"
      },
      "outputs": [],
      "source": [
        "\n",
        "def tokenize_function(examples):\n",
        "    try:\n",
        "      # print(examples)\n",
        "      examples_text = [text or \" \" for text in examples[\"text\"]]\n",
        "      return tokenizer(examples_text, padding=\"max_length\", truncation=True,max_length=chunk_size)\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      # # print(examples)\n",
        "      # return tokenizer(examples, padding=\"max_length\", truncation=True,max_length=512)\n",
        "\n",
        "      \n",
        "      # print(\"Exception in tokenizer : %s \" %e)\n",
        "\n",
        "def tokenize_files(files_dir, limit=1):\n",
        "    tokenized_dir = os.path.join(files_dir, \"tokenized_dataset_{0}\".format(chunk_size))\n",
        "    os.makedirs(tokenized_dir, exist_ok=True)\n",
        "    print(\"Tokenzed data director %s \" %tokenized_dir)\n",
        "    print(\"Tokenizing files in %s \" %files_dir)\n",
        "    chunk_files = sorted(os.listdir(files_dir))\n",
        "    tokenized_files = os.listdir(tokenized_dir)\n",
        "    print(\"Tokenized files %s \" %tokenized_files)\n",
        "    stop_limit = 0\n",
        "    for chunk in chunk_files:\n",
        "      try:\n",
        "        if stop_limit > limit:\n",
        "          break\n",
        "        chunk_file = os.path.join(files_dir, chunk)\n",
        "        tokenized_file_name = \"tokenized_{0}\".format(chunk)\n",
        "        tokenized_file = os.path.join(tokenized_dir, tokenized_file_name)\n",
        "        if tokenized_file_name in tokenized_files:\n",
        "          print(\"Token data set already present. Skipping.\")\n",
        "          continue\n",
        "\n",
        "        # chunk_data = pd.read_csv(chunk_file)\n",
        "        tokenize_csv(chunk_file, tokenized_dir)\n",
        "        stop_limit = stop_limit + 1\n",
        "        \n",
        "        # gc.collect()\n",
        "\n",
        "\n",
        "      except Exception as error:\n",
        "        print(\"Tokenising error in file %s\" %error)\n",
        "\n",
        "\n",
        "    return tokenized_dir\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C61XcBBos2t3",
        "outputId": "f2ca741b-3af7-40da-a14e-5bc20a4d03d9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjs12962\u001b[0m (\u001b[33mclinical-context\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230505_130301-d6np6nqg</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/clinical-context/clinical-context/runs/d6np6nqg' target=\"_blank\">grievous-astromech-19</a></strong> to <a href='https://wandb.ai/clinical-context/clinical-context' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/clinical-context/clinical-context' target=\"_blank\">https://wandb.ai/clinical-context/clinical-context</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/clinical-context/clinical-context/runs/d6np6nqg' target=\"_blank\">https://wandb.ai/clinical-context/clinical-context/runs/d6np6nqg</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenzed data director /content/drive/MyDrive/ClinicalContext/discharge_chunks/tokenized_dataset_128 \n",
            "Tokenizing files in /content/drive/MyDrive/ClinicalContext/discharge_chunks \n",
            "Tokenized files ['tokenized_chunk_0.csv_0', 'tokenized_chunk_0.csv_1', 'tokenized_chunk_1.csv_0', 'tokenized_chunk_1.csv_1', 'tokenized_chunk_10.csv_0', 'tokenized_chunk_10.csv_1', 'tokenized_chunk_11.csv_0', 'tokenized_chunk_11.csv_1', 'tokenized_chunk_12.csv_0', 'tokenized_chunk_12.csv_1', 'tokenized_chunk_13.csv_0', 'tokenized_chunk_13.csv_1', 'tokenized_chunk_14.csv_0', 'tokenized_chunk_14.csv_1', 'tokenized_chunk_15.csv_0', 'tokenized_chunk_2.csv_0', 'tokenized_chunk_2.csv_1', 'tokenized_chunk_3.csv_0', 'tokenized_chunk_3.csv_1', 'tokenized_chunk_4.csv_0', 'tokenized_chunk_4.csv_1', 'tokenized_chunk_5.csv_0', 'tokenized_chunk_5.csv_1', 'tokenized_chunk_6.csv_0', 'tokenized_chunk_6.csv_1', 'tokenized_chunk_7.csv_0', 'tokenized_chunk_7.csv_1', 'tokenized_chunk_8.csv_0', 'tokenized_chunk_8.csv_1', 'tokenized_chunk_9.csv_0', 'tokenized_chunk_9.csv_1'] \n",
            "Tokenizing start : file /content/drive/MyDrive/ClinicalContext/discharge_chunks/chunk_0.csv \n",
            "Token data set already present for chunk. Skipping.\n",
            "Token data set already present for chunk. Skipping.\n",
            "Tokenizing complete : file /content/drive/MyDrive/ClinicalContext/discharge_chunks/chunk_0.csv\n",
            "Tokenizing start : file /content/drive/MyDrive/ClinicalContext/discharge_chunks/chunk_1.csv \n",
            "Token data set already present for chunk. Skipping.\n",
            "Token data set already present for chunk. Skipping.\n",
            "Tokenizing complete : file /content/drive/MyDrive/ClinicalContext/discharge_chunks/chunk_1.csv\n",
            "Tokenizing start : file /content/drive/MyDrive/ClinicalContext/discharge_chunks/chunk_10.csv \n",
            "Token data set already present for chunk. Skipping.\n",
            "Token data set already present for chunk. Skipping.\n",
            "Tokenizing complete : file /content/drive/MyDrive/ClinicalContext/discharge_chunks/chunk_10.csv\n",
            "Tokenizing start : file /content/drive/MyDrive/ClinicalContext/discharge_chunks/chunk_11.csv \n",
            "Token data set already present for chunk. Skipping.\n",
            "Token data set already present for chunk. Skipping.\n",
            "Tokenizing complete : file /content/drive/MyDrive/ClinicalContext/discharge_chunks/chunk_11.csv\n",
            "Tokenizing start : file /content/drive/MyDrive/ClinicalContext/discharge_chunks/chunk_12.csv \n",
            "Token data set already present for chunk. Skipping.\n",
            "Token data set already present for chunk. Skipping.\n",
            "Tokenizing complete : file /content/drive/MyDrive/ClinicalContext/discharge_chunks/chunk_12.csv\n",
            "Tokenizing start : file /content/drive/MyDrive/ClinicalContext/discharge_chunks/chunk_13.csv \n",
            "Token data set already present for chunk. Skipping.\n",
            "Token data set already present for chunk. Skipping.\n",
            "Tokenizing complete : file /content/drive/MyDrive/ClinicalContext/discharge_chunks/chunk_13.csv\n",
            "Tokenizing start : file /content/drive/MyDrive/ClinicalContext/discharge_chunks/chunk_14.csv \n",
            "Token data set already present for chunk. Skipping.\n",
            "Token data set already present for chunk. Skipping.\n",
            "Tokenizing complete : file /content/drive/MyDrive/ClinicalContext/discharge_chunks/chunk_14.csv\n",
            "Tokenizing start : file /content/drive/MyDrive/ClinicalContext/discharge_chunks/chunk_15.csv \n",
            "Token data set already present for chunk. Skipping.\n",
            "Tokenizing complete : file /content/drive/MyDrive/ClinicalContext/discharge_chunks/chunk_15.csv\n",
            "Tokenizing start : file /content/drive/MyDrive/ClinicalContext/discharge_chunks/chunk_2.csv \n",
            "Token data set already present for chunk. Skipping.\n",
            "Token data set already present for chunk. Skipping.\n",
            "Tokenizing complete : file /content/drive/MyDrive/ClinicalContext/discharge_chunks/chunk_2.csv\n",
            "Tokenizing start : file /content/drive/MyDrive/ClinicalContext/discharge_chunks/chunk_3.csv \n",
            "Token data set already present for chunk. Skipping.\n",
            "Token data set already present for chunk. Skipping.\n",
            "Tokenizing complete : file /content/drive/MyDrive/ClinicalContext/discharge_chunks/chunk_3.csv\n",
            "Tokenizing start : file /content/drive/MyDrive/ClinicalContext/discharge_chunks/chunk_4.csv \n",
            "Token data set already present for chunk. Skipping.\n",
            "Token data set already present for chunk. Skipping.\n",
            "Tokenizing complete : file /content/drive/MyDrive/ClinicalContext/discharge_chunks/chunk_4.csv\n",
            "Tokenizing start : file /content/drive/MyDrive/ClinicalContext/discharge_chunks/chunk_5.csv \n",
            "Token data set already present for chunk. Skipping.\n",
            "Token data set already present for chunk. Skipping.\n",
            "Tokenizing complete : file /content/drive/MyDrive/ClinicalContext/discharge_chunks/chunk_5.csv\n",
            "Tokenizing start : file /content/drive/MyDrive/ClinicalContext/discharge_chunks/chunk_6.csv \n",
            "Token data set already present for chunk. Skipping.\n",
            "Token data set already present for chunk. Skipping.\n",
            "Tokenizing complete : file /content/drive/MyDrive/ClinicalContext/discharge_chunks/chunk_6.csv\n",
            "Tokenizing start : file /content/drive/MyDrive/ClinicalContext/discharge_chunks/chunk_7.csv \n",
            "Token data set already present for chunk. Skipping.\n",
            "Token data set already present for chunk. Skipping.\n",
            "Tokenizing complete : file /content/drive/MyDrive/ClinicalContext/discharge_chunks/chunk_7.csv\n",
            "Tokenizing start : file /content/drive/MyDrive/ClinicalContext/discharge_chunks/chunk_8.csv \n",
            "Token data set already present for chunk. Skipping.\n",
            "Token data set already present for chunk. Skipping.\n",
            "Tokenizing complete : file /content/drive/MyDrive/ClinicalContext/discharge_chunks/chunk_8.csv\n",
            "Tokenizing start : file /content/drive/MyDrive/ClinicalContext/discharge_chunks/chunk_9.csv \n",
            "Token data set already present for chunk. Skipping.\n",
            "Token data set already present for chunk. Skipping.\n",
            "Tokenizing complete : file /content/drive/MyDrive/ClinicalContext/discharge_chunks/chunk_9.csv\n",
            "Tokenizing start : file /content/drive/MyDrive/ClinicalContext/discharge_chunks/tokenized_data \n",
            "Tokenising error in file [Errno 21] Is a directory: '/content/drive/MyDrive/ClinicalContext/discharge_chunks/tokenized_data'\n",
            "Tokenizing start : file /content/drive/MyDrive/ClinicalContext/discharge_chunks/tokenized_dataset \n",
            "Tokenising error in file [Errno 21] Is a directory: '/content/drive/MyDrive/ClinicalContext/discharge_chunks/tokenized_dataset'\n",
            "Tokenizing start : file /content/drive/MyDrive/ClinicalContext/discharge_chunks/tokenized_dataset_128 \n",
            "Tokenising error in file [Errno 21] Is a directory: '/content/drive/MyDrive/ClinicalContext/discharge_chunks/tokenized_dataset_128'\n"
          ]
        }
      ],
      "source": [
        "#Tokenizing code\n",
        "\n",
        "wandb.init(\n",
        "      # set the wandb project where this run will be logged\n",
        "      project=\"clinical-context\",\n",
        "      \n",
        "      # track hyperparameters and run metadata\n",
        "      config={\n",
        "      \"task\" : \"tokenizer\",\n",
        "      \"dataset\" : \"discharge-notes\"\n",
        "      }\n",
        "  )\n",
        "\n",
        "chunk_size = 128\n",
        "model_name = \"dmis-lab/biobert-base-cased-v1.1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "dataset_dir = \"/content/drive/MyDrive/ClinicalContext/discharge_chunks\"\n",
        "tokenized_dir = tokenize_files(dataset_dir, limit=100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5JdMboZIs00"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCD-6e0Wt3dS",
        "outputId": "94664ebf-8c79-4303-88f1-ff8473f5b5d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
            "\n",
            "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at Jaikar/biobert-base-cased-v1.1-finetuned-clinical-context.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:nnusmv6c) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">scruffy-looking-council-20</strong> at: <a href='https://wandb.ai/clinical-context/clinical-context/runs/nnusmv6c' target=\"_blank\">https://wandb.ai/clinical-context/clinical-context/runs/nnusmv6c</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230505_130357-nnusmv6c/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:nnusmv6c). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230505_131738-rd3afavy</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/clinical-context/clinical-context/runs/rd3afavy' target=\"_blank\">carbonite-wars-21</a></strong> to <a href='https://wandb.ai/clinical-context/clinical-context' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/clinical-context/clinical-context' target=\"_blank\">https://wandb.ai/clinical-context/clinical-context</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/clinical-context/clinical-context/runs/rd3afavy' target=\"_blank\">https://wandb.ai/clinical-context/clinical-context/runs/rd3afavy</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached split indices for dataset at /content/drive/MyDrive/ClinicalContext/discharge_chunks/tokenized_dataset_128/tokenized_chunk_11.csv_0/cache-a3c4e6118aa2b416.arrow and /content/drive/MyDrive/ClinicalContext/discharge_chunks/tokenized_dataset_128/tokenized_chunk_11.csv_0/cache-2c79a4859575f975.arrow\n",
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:388: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
            "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
            "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
            "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
            "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
            "  warnings.warn(\n",
            "WARNING:datasets.arrow_dataset:Loading cached split indices for dataset at /content/drive/MyDrive/ClinicalContext/discharge_chunks/tokenized_dataset_128/tokenized_chunk_11.csv_1/cache-b9951a7aea789389.arrow and /content/drive/MyDrive/ClinicalContext/discharge_chunks/tokenized_dataset_128/tokenized_chunk_11.csv_1/cache-899824dd4964a0d8.arrow\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n",
            "/content/Jaikar/biobert-base-cased-v1.1-finetuned-clinical-context is already a clone of https://huggingface.co/Jaikar/biobert-base-cased-v1.1-finetuned-clinical-context. Make sure you pull the latest changes with `repo.git_pull()`.\n",
            "WARNING:huggingface_hub.repository:/content/Jaikar/biobert-base-cased-v1.1-finetuned-clinical-context is already a clone of https://huggingface.co/Jaikar/biobert-base-cased-v1.1-finetuned-clinical-context. Make sure you pull the latest changes with `repo.git_pull()`.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model in the h5py format. The model will be saved as as an W&B Artifact in the 'tf' format.\n",
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, predictions_layer_call_fn while saving (showing 5 of 424). These functions will not be directly callable after loading.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/wandb/run-20230505_131738-rd3afavy/files/model-best)... Done. 6.9s\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:388: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
            "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
            "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
            "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
            "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
            "  warnings.warn(\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n",
            "/content/Jaikar/biobert-base-cased-v1.1-finetuned-clinical-context is already a clone of https://huggingface.co/Jaikar/biobert-base-cased-v1.1-finetuned-clinical-context. Make sure you pull the latest changes with `repo.git_pull()`.\n",
            "WARNING:huggingface_hub.repository:/content/Jaikar/biobert-base-cased-v1.1-finetuned-clinical-context is already a clone of https://huggingface.co/Jaikar/biobert-base-cased-v1.1-finetuned-clinical-context. Make sure you pull the latest changes with `repo.git_pull()`.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model in the h5py format. The model will be saved as as an W&B Artifact in the 'tf' format.\n",
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, predictions_layer_call_fn while saving (showing 5 of 424). These functions will not be directly callable after loading.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/wandb/run-20230505_131738-rd3afavy/files/model-best)... Done. 6.9s\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:388: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
            "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
            "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
            "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
            "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
            "  warnings.warn(\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n",
            "/content/Jaikar/biobert-base-cased-v1.1-finetuned-clinical-context is already a clone of https://huggingface.co/Jaikar/biobert-base-cased-v1.1-finetuned-clinical-context. Make sure you pull the latest changes with `repo.git_pull()`.\n",
            "WARNING:huggingface_hub.repository:/content/Jaikar/biobert-base-cased-v1.1-finetuned-clinical-context is already a clone of https://huggingface.co/Jaikar/biobert-base-cased-v1.1-finetuned-clinical-context. Make sure you pull the latest changes with `repo.git_pull()`.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model in the h5py format. The model will be saved as as an W&B Artifact in the 'tf' format.\n",
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, predictions_layer_call_fn while saving (showing 5 of 424). These functions will not be directly callable after loading.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/wandb/run-20230505_131738-rd3afavy/files/model-best)... Done. 6.8s\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:388: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
            "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
            "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
            "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
            "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
            "  warnings.warn(\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n",
            "/content/Jaikar/biobert-base-cased-v1.1-finetuned-clinical-context is already a clone of https://huggingface.co/Jaikar/biobert-base-cased-v1.1-finetuned-clinical-context. Make sure you pull the latest changes with `repo.git_pull()`.\n",
            "WARNING:huggingface_hub.repository:/content/Jaikar/biobert-base-cased-v1.1-finetuned-clinical-context is already a clone of https://huggingface.co/Jaikar/biobert-base-cased-v1.1-finetuned-clinical-context. Make sure you pull the latest changes with `repo.git_pull()`.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model in the h5py format. The model will be saved as as an W&B Artifact in the 'tf' format.\n",
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, predictions_layer_call_fn while saving (showing 5 of 424). These functions will not be directly callable after loading.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/wandb/run-20230505_131738-rd3afavy/files/model-best)... Done. 7.4s\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:388: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
            "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
            "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
            "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
            "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
            "  warnings.warn(\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n",
            "/content/Jaikar/biobert-base-cased-v1.1-finetuned-clinical-context is already a clone of https://huggingface.co/Jaikar/biobert-base-cased-v1.1-finetuned-clinical-context. Make sure you pull the latest changes with `repo.git_pull()`.\n",
            "WARNING:huggingface_hub.repository:/content/Jaikar/biobert-base-cased-v1.1-finetuned-clinical-context is already a clone of https://huggingface.co/Jaikar/biobert-base-cased-v1.1-finetuned-clinical-context. Make sure you pull the latest changes with `repo.git_pull()`.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model in the h5py format. The model will be saved as as an W&B Artifact in the 'tf' format.\n",
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, predictions_layer_call_fn while saving (showing 5 of 424). These functions will not be directly callable after loading.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/wandb/run-20230505_131738-rd3afavy/files/model-best)... Done. 6.9s\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:388: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
            "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
            "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
            "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
            "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
            "  warnings.warn(\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n",
            "/content/Jaikar/biobert-base-cased-v1.1-finetuned-clinical-context is already a clone of https://huggingface.co/Jaikar/biobert-base-cased-v1.1-finetuned-clinical-context. Make sure you pull the latest changes with `repo.git_pull()`.\n",
            "WARNING:huggingface_hub.repository:/content/Jaikar/biobert-base-cased-v1.1-finetuned-clinical-context is already a clone of https://huggingface.co/Jaikar/biobert-base-cased-v1.1-finetuned-clinical-context. Make sure you pull the latest changes with `repo.git_pull()`.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model in the h5py format. The model will be saved as as an W&B Artifact in the 'tf' format.\n",
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, predictions_layer_call_fn while saving (showing 5 of 424). These functions will not be directly callable after loading.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/wandb/run-20230505_131738-rd3afavy/files/model-best)... Done. 7.5s\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:388: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
            "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
            "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
            "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
            "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
            "  warnings.warn(\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n",
            "/content/Jaikar/biobert-base-cased-v1.1-finetuned-clinical-context is already a clone of https://huggingface.co/Jaikar/biobert-base-cased-v1.1-finetuned-clinical-context. Make sure you pull the latest changes with `repo.git_pull()`.\n",
            "WARNING:huggingface_hub.repository:/content/Jaikar/biobert-base-cased-v1.1-finetuned-clinical-context is already a clone of https://huggingface.co/Jaikar/biobert-base-cased-v1.1-finetuned-clinical-context. Make sure you pull the latest changes with `repo.git_pull()`.\n"
          ]
        }
      ],
      "source": [
        "from transformers import create_optimizer\n",
        "from transformers.keras_callbacks import PushToHubCallback\n",
        "import tensorflow as tf\n",
        "from transformers import AutoModelForMaskedLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from transformers import TFAutoModelForMaskedLM\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "import math\n",
        "\n",
        "\n",
        "chunk_size = 128\n",
        "\n",
        "\n",
        "model_name = \"Jaikar/biobert-base-cased-v1.1-finetuned-clinical-context\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Jaikar/biobert-base-cased-v1.1-finetuned-clinical-context\")\n",
        "\n",
        "model = TFAutoModelForMaskedLM.from_pretrained(\"Jaikar/biobert-base-cased-v1.1-finetuned-clinical-context\")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
        "\n",
        "\n",
        "dataset_dir = \"/content/drive/MyDrive/ClinicalContext/discharge_chunks\"\n",
        "\n",
        "tokenized_dir = os.path.join(dataset_dir, \"tokenized_dataset_{0}\".format(chunk_size))\n",
        "\n",
        "\n",
        "tokenized_files = os.listdir(tokenized_dir)[6:]\n",
        "\n",
        "# first_file = [tokenized_files[0]]\n",
        "# first_file.extend(tokenized_files[6:])\n",
        "# tokenized_files = first_file\n",
        "wandb.init(\n",
        "      # set the wandb project where this run will be logged\n",
        "      project=\"clinical-context\",\n",
        "      \n",
        "      # track hyperparameters and run metadata\n",
        "      config={\n",
        "      \"chunk_size\": 128,\n",
        "      \"architecture\": \"BIOBERT\",\n",
        "      \"dataset\": \"MIMIC-4\",\n",
        "      \"epochs\": 1,\n",
        "      \"weight_decay_rate\" : 0.01\n",
        "      }\n",
        "  )\n",
        "\n",
        "for idx in range(len(tokenized_files[6:])):\n",
        "  print(\"Training on shard %s \" %tokenized_files[idx] )\n",
        "  # wandb.log(\"Training on shard %s \" %tokenized_files[idx] )\n",
        "  \n",
        "  tokenized_shard_path = os.path.join(tokenized_dir,tokenized_files[idx])\n",
        "\n",
        "  tokenized_shard = datasets.load_from_disk(tokenized_shard_path)\n",
        "\n",
        "  len(tokenized_shard)\n",
        "\n",
        "  tokenized_shard.remove_columns(['text'])\n",
        "  tokenized_shard = tokenized_shard.add_column('labels', tokenized_shard['input_ids'] )\n",
        "  downsampled_dataset = tokenized_shard.train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "  if idx == 0:\n",
        "    print(\"Creating train and eval\")\n",
        "    tf_train_dataset = model.prepare_tf_dataset(\n",
        "        downsampled_dataset[\"train\"],\n",
        "        collate_fn=data_collator,\n",
        "        shuffle=True,\n",
        "        batch_size=32,\n",
        "    )\n",
        "\n",
        "    tf_eval_dataset = model.prepare_tf_dataset(\n",
        "        downsampled_dataset[\"test\"],\n",
        "        collate_fn=data_collator,\n",
        "        shuffle=False,\n",
        "        batch_size=32,\n",
        "    )\n",
        "    continue\n",
        "\n",
        "  else:\n",
        "    tf_train_dataset = model.prepare_tf_dataset(\n",
        "        tokenized_shard,\n",
        "        collate_fn=data_collator,\n",
        "        shuffle=True,\n",
        "        batch_size=32,\n",
        "    )\n",
        "\n",
        "\n",
        "  num_train_steps = len(tf_train_dataset)\n",
        "  optimizer, schedule = create_optimizer(\n",
        "      init_lr=2e-5,\n",
        "      num_warmup_steps=1_000,\n",
        "      num_train_steps=num_train_steps,\n",
        "      weight_decay_rate=0.01,\n",
        "  )\n",
        "  model.compile(optimizer=optimizer)\n",
        "\n",
        "  # Train in mixed-precision float16\n",
        "  tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "\n",
        "  # model_name = model_name.split(\"/\")[-1]\n",
        "  callback = PushToHubCallback(\n",
        "      output_dir=f\"{model_name}\", tokenizer=tokenizer\n",
        "  )\n",
        "\n",
        "  \n",
        "  # eval_loss = model.evaluate(tf_eval_dataset)\n",
        "  # print(f\"Perplexity before training : {math.exp(eval_loss):.2f}\")\n",
        "  model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback, WandbCallback()], epochs=1)\n",
        "  eval_loss = model.evaluate(tf_eval_dataset)\n",
        "  print(f\"perplexity %s {math.exp(eval_loss):.2f} shard idx {idx}\")\n",
        "  wandb.log({\"perplexity\" : math.exp(eval_loss), \"shard idx\" : idx})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FicSQSFOuiID"
      },
      "outputs": [],
      "source": [
        "eval_loss = model.evaluate(tf_eval_dataset)\n",
        "print(f\"Perplexity: {math.exp(eval_loss):.2f}\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4kV_KlCABoX"
      },
      "source": [
        "### PyTorch"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "72f0268a91584ce181c03f70a7365456": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9fed38a441f14e91aa2a854436870271",
              "IPY_MODEL_1eb1f3ae5e914d77a5113bb6d05c248e",
              "IPY_MODEL_d624042b78a04865ab8ece51a005ebc9",
              "IPY_MODEL_99de6fee6d594e6eaeadf07888acc94c",
              "IPY_MODEL_222fd374464941049dd750aaa8facf0f"
            ],
            "layout": "IPY_MODEL_bdd2d05fbeec4f58b91d8c0c71fa1f25"
          }
        },
        "9fed38a441f14e91aa2a854436870271": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34bd1c6f0ab64c19bd297efae7e9e2d4",
            "placeholder": "​",
            "style": "IPY_MODEL_9ac0ee618ba24517a237792cca0b1c2b",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "1eb1f3ae5e914d77a5113bb6d05c248e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_f01f29e3fc174f39a44e2a539cf96043",
            "placeholder": "​",
            "style": "IPY_MODEL_522bbc97683e41d2b3d1e838ea372694",
            "value": ""
          }
        },
        "d624042b78a04865ab8ece51a005ebc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_1f0d1a65cd7140e5ac95894b25ea836b",
            "style": "IPY_MODEL_3b85b6f868f3410ebae637b495f85f58",
            "value": true
          }
        },
        "99de6fee6d594e6eaeadf07888acc94c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_61c20409fa784116a4e74ebe7a72f1fc",
            "style": "IPY_MODEL_ad3c90436af44019ba1813ea5e37f9d7",
            "tooltip": ""
          }
        },
        "222fd374464941049dd750aaa8facf0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0abfa64498674d9cbbdf4a05da3c3381",
            "placeholder": "​",
            "style": "IPY_MODEL_918534600cb54d3288848fac2a20baf7",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "bdd2d05fbeec4f58b91d8c0c71fa1f25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "34bd1c6f0ab64c19bd297efae7e9e2d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ac0ee618ba24517a237792cca0b1c2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f01f29e3fc174f39a44e2a539cf96043": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "522bbc97683e41d2b3d1e838ea372694": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f0d1a65cd7140e5ac95894b25ea836b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b85b6f868f3410ebae637b495f85f58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61c20409fa784116a4e74ebe7a72f1fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad3c90436af44019ba1813ea5e37f9d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "0abfa64498674d9cbbdf4a05da3c3381": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "918534600cb54d3288848fac2a20baf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}